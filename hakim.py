import cv2
import numpy as np
import streamlit as st
from ultralytics import YOLO

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ YOLOv8 Pose
model = YOLO("yolov8n-pose.pt")

# Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø© Streamlit
st.set_page_config(page_title="Real-Time Pose Estimation", layout="wide")
st.title("ğŸ‘ï¸ ÙƒØ´Ù Ù…ÙØ§ØµÙ„ Ø§Ù„Ø¬Ø³Ù… Ø§Ù„Ø­ÙŠ - YOLOv8 Pose")
st.markdown("---")

# Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
option = st.radio("Ø§Ø®ØªØ± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:", ["ğŸ“· Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©", "ğŸ“ Ø±ÙØ¹ ØµÙˆØ±Ø©"], horizontal=True)

# Ø¯Ø§Ù„Ø© Ù„Ø±Ø³Ù… Ø§Ù„Ù…ÙØ§ØµÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·
def draw_custom_joints(frame, keypoints, conf_threshold=0.5):
    # Ø§Ù„Ù…ÙØ§ØµÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙ‚Ø·
    JOINT_INDICES = {
        'shoulder': [5, 6], 
        'elbow': [7, 8], 
        'wrist': [9, 10],
        'hip': [11, 12], 
        'knee': [13, 14]
    }

    COLORS = {
        'shoulder': (0, 255, 0),    # Ø£Ø®Ø¶Ø±
        'elbow': (255, 0, 0),       # Ø£Ø²Ø±Ù‚
        'wrist': (0, 0, 255),       # Ø£Ø­Ù…Ø±
        'hip': (255, 255, 0),       # Ø³Ù…Ø§ÙˆÙŠ
        'knee': (0, 255, 255)       # Ø£ØµÙØ±
    }

    for joint_type, indices in JOINT_INDICES.items():
        for idx in indices:
            kp = keypoints[idx]
            if kp[2] >= conf_threshold:  # Ø§Ù„Ø«Ù‚Ø©
                x, y = int(kp[0]), int(kp[1])
                cv2.circle(frame, (x, y), 6, COLORS[joint_type], -1)

    # Ø®Ø·ÙˆØ· Ø§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…ÙØ§ØµÙ„
    connections = [(5,7), (6,8), (7,9), (8,10), (11,13), (12,14)]  
    for (start, end) in connections:
        kp_start, kp_end = keypoints[start], keypoints[end]
        if kp_start[2] >= conf_threshold and kp_end[2] >= conf_threshold:
            start_pt = (int(kp_start[0]), int(kp_start[1]))
            end_pt = (int(kp_end[0]), int(kp_end[1]))
            cv2.line(frame, start_pt, end_pt, (255, 255, 255), 2)

    return frame

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø·Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… YOLO
def process_frame(frame):
    results = model(frame)
    if results[0].keypoints is not None:
        keypoints = results[0].keypoints.xy.cpu().numpy()[0]  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø·
        return draw_custom_joints(frame, keypoints)
    return frame

# ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
if option == "ğŸ“· Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©":
    live_cam = st.checkbox("âœ… ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
    
    if live_cam:
        cap = cv2.VideoCapture(0)  # ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
        
        if not cap.isOpened():
            st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ù…ÙŠØ±Ø§!")
        else:
            stframe = st.empty()
            while live_cam:
                ret, frame = cap.read()
                if not ret:
                    st.error("âŒ ØªØ¹Ø°Ø± ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§!")
                    break
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                processed = process_frame(frame)
                stframe.image(processed, use_column_width=True)
            
            cap.release()

# Ø±ÙØ¹ ØµÙˆØ±Ø© ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§
elif option == "ğŸ“ Ø±ÙØ¹ ØµÙˆØ±Ø©":
    uploaded_file = st.file_uploader("ğŸ–¼ï¸ Ø§Ø®ØªØ± ØµÙˆØ±Ø©", type=["jpg", "png", "jpeg"])
    if uploaded_file is not None:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        frame = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        processed = process_frame(frame)
        st.image(processed, caption="ğŸ” Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù…ÙØ§ØµÙ„", use_column_width=True)
